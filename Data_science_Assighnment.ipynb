{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrfidDzUdylj",
        "outputId": "a6b2d8e2-159f-481c-fecd-bb7b01be961b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import ne_chunk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can read our case files either from local storage or by mounting them in google Drive to turning into Google colab\n",
        "sentence=\"Abhay Manohar Sapre,Delay in filing special leave petition is condoned. Leave granted.This appeal is filed against the final judgment and order dated 08.04.2015 of the High Court of Judicature at Patna in CWJC No. 5402 of 2015 whereby the High Court while disposing of the Appellant's writ petition granted liberty to file representation to the National Thermal Power Corporation (NTPC) but at the same time passed an order that the Appellants will have no liberty to move the High Court again for the same cause of action raised therein.\"\n"
      ],
      "metadata": {
        "id": "jUA4o8ROhPNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "sent_tokens=word_tokenize(sentence)\n",
        "sent_tags=nltk.pos_tag(sent_tokens)\n"
      ],
      "metadata": {
        "id": "dodODHPJheK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sent_output=ne_chunk(sent_tags)\n",
        "#print(sent_output)\n",
        "print(sent_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJrWKox2h5qX",
        "outputId": "04090c81-20c9-4739-9089-5f036ba62ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Abhay', 'NNP'), ('Manohar', 'NNP'), ('Sapre', 'NNP'), (',', ','), ('Delay', 'NNP'), ('in', 'IN'), ('filing', 'VBG'), ('special', 'JJ'), ('leave', 'JJ'), ('petition', 'NN'), ('is', 'VBZ'), ('condoned', 'VBN'), ('.', '.'), ('Leave', 'VB'), ('granted.This', 'JJ'), ('appeal', 'NN'), ('is', 'VBZ'), ('filed', 'VBN'), ('against', 'IN'), ('the', 'DT'), ('final', 'JJ'), ('judgment', 'NN'), ('and', 'CC'), ('order', 'NN'), ('dated', 'VBD'), ('08.04.2015', 'CD'), ('of', 'IN'), ('the', 'DT'), ('High', 'NNP'), ('Court', 'NNP'), ('of', 'IN'), ('Judicature', 'NNP'), ('at', 'IN'), ('Patna', 'NNP'), ('in', 'IN'), ('CWJC', 'NNP'), ('No', 'NNP'), ('.', '.'), ('5402', 'CD'), ('of', 'IN'), ('2015', 'CD'), ('whereby', 'IN'), ('the', 'DT'), ('High', 'NNP'), ('Court', 'NNP'), ('while', 'IN'), ('disposing', 'VBG'), ('of', 'IN'), ('the', 'DT'), ('Appellant', 'NNP'), (\"'s\", 'POS'), ('writ', 'NN'), ('petition', 'NN'), ('granted', 'VBD'), ('liberty', 'NN'), ('to', 'TO'), ('file', 'VB'), ('representation', 'NN'), ('to', 'TO'), ('the', 'DT'), ('National', 'NNP'), ('Thermal', 'NNP'), ('Power', 'NNP'), ('Corporation', 'NNP'), ('(', '('), ('NTPC', 'NNP'), (')', ')'), ('but', 'CC'), ('at', 'IN'), ('the', 'DT'), ('same', 'JJ'), ('time', 'NN'), ('passed', 'VBD'), ('an', 'DT'), ('order', 'NN'), ('that', 'IN'), ('the', 'DT'), ('Appellants', 'NNPS'), ('will', 'MD'), ('have', 'VB'), ('no', 'DT'), ('liberty', 'NN'), ('to', 'TO'), ('move', 'VB'), ('the', 'DT'), ('High', 'NNP'), ('Court', 'NNP'), ('again', 'RB'), ('for', 'IN'), ('the', 'DT'), ('same', 'JJ'), ('cause', 'NN'), ('of', 'IN'), ('action', 'NN'), ('raised', 'VBN'), ('therein', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_words=[]\n",
        "for each_pos in sent_tags:\n",
        "  if each_pos[1] =='NNS':\n",
        "    main_words.append(each_pos[0])\n",
        "  elif each_pos[1]=='NNP':\n",
        "    main_words.append(each_pos[0])\n",
        "  #elif each_pos[1]=='NN':\n",
        "   # main_words.append(each_pos[0])\n"
      ],
      "metadata": {
        "id": "Zt_f6Vjoph03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(', '.join(main_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNhYKfo5qS5i",
        "outputId": "732c36cb-f8f3-444f-aa9c-c21147567806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abhay, Manohar, Sapre, Delay, High, Court, Judicature, Patna, CWJC, No, High, Court, Appellant, National, Thermal, Power, Corporation, NTPC, High, Court\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import *\n",
        "sent = \"India is a republic nation. We are proud Indians\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sent)\n",
        "vocab = sorted(set(tokens))\n",
        "print(vocab) #Prints ['.', 'India', 'Indians', 'We', 'a', 'are', 'country', 'is', 'proud', 'republic']\n",
        "vocab_wo_punct=[]\n",
        "from nltk import pos_tag\n",
        "pos_list = pos_tag(vocab_wo_punct)\n",
        "print(pos_list)\n",
        "\"\"\" Prints [('India', 'NNP'), ('Indians', 'NNPS'), ('We', 'PRP'), ('a', 'DT'), \n",
        "            ('are', 'VBP'), ('country', 'NN'), ('is', 'VBZ'), ('proud', 'JJ'), \n",
        "            ('republic', 'JJ'), ('India', 'NNP')] \"\"\"\n",
        "\n",
        "from string import punctuation\n",
        "vocab_wo_punct=[]\n",
        "for i in vocab:\n",
        "    if i not in punctuation:\n",
        "        vocab_wo_punct.append(i)\n",
        "print(vocab_wo_punct) #Prints ['India', 'Indians', 'We', 'a', 'are', 'country', 'is', 'proud', 'republic']\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemObj = SnowballStemmer(\"english\")\n",
        "stemObj.stem(\"Studying\") #Prints 'studi'\n",
        "stemmed_vocab=[]\n",
        "stemObj = SnowballStemmer(\"english\")\n",
        "for i in vocab_no_punct:\n",
        "    stemmed_vocab.append(stemObj.stem(i))\n",
        "print(stemmed_vocab) #Prints ['india', 'indian', 'we', 'a', 'are', 'countri', 'is', 'proud', 'republ'] \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "wo_stop_words = []\n",
        "stop_words_set = set(stopwords.words(\"english\"))\n",
        "for i in vocab_no_punct:\n",
        "    if i not in stop_words_set:\n",
        "        wo_stop_words.append(i)\n",
        "print(wo_stop_words) #Prints ['India', 'Indians', 'We', 'country', 'proud', 'republic']  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "mK-OrHl1N0R0",
        "outputId": "00e95028-8441-4272-f264-82c23498e47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.', 'India', 'Indians', 'We', 'a', 'are', 'is', 'nation', 'proud', 'republic']\n",
            "[]\n",
            "['India', 'Indians', 'We', 'a', 'are', 'is', 'nation', 'proud', 'republic']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-7a6e312bec16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mstemmed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mstemObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_no_punct\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mstemmed_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_vocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Prints ['india', 'indian', 'we', 'a', 'are', 'countri', 'is', 'proud', 'republ']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab_no_punct' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "#use 2 for bigrams\n",
        "bigrams = ngrams(vocab_no_punct,2)\n",
        "print(list(bigrams)) \n",
        "#Prints [('India', 'Indians'), ('Indians', 'We'), ('We', 'a'), ('a', 'are'), ('are', 'country'), \n",
        "         ('country', 'is'), ('is', 'proud'), ('proud', 'republic')] \n",
        "#use 3 for trigrams\n",
        "trigrams = ngrams(vocab_no_punct,3)\n",
        "print(list(trigrams)) \n",
        "[('India', 'Indians', 'We'), ('Indians', 'We', 'a'), ('We', 'a', 'are'), ('a', 'are', 'country'), \n",
        " ('are', 'country', 'is'), ('country', 'is', 'proud'), ('is', 'proud', 'republic')]\n"
      ],
      "metadata": {
        "id": "wNbyWAGGP6xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"I saw John coming. He was with Mary. I talked to John and Mary. \\\n",
        "John said he met Mary on the way. John and Mary were going to school.\"\n",
        "print(nltk.FreqDist(nltk.word_tokenize(text)))\n",
        "#Prints FreqDist({'.': 5, 'Mary': 4, 'John': 4, 'I': 2, 'to': 2, 'and': 2, 'the': 1, 'was': 1, 'were': 1, 'school': 1, ...})\n",
        "nltk.FreqDist(text.split()).plot()\n"
      ],
      "metadata": {
        "id": "BX_Dtp5qM9Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "#use 2 for bigrams\n",
        "bigrams = ngrams(vocab_no_punct,2)\n",
        "print(list(bigrams)) \n",
        "#Prints [('India', 'Indians'), ('Indians', 'We'), ('We', 'a'), ('a', 'are'), ('are', 'country'), \n",
        "         ('country', 'is'), ('is', 'proud'), ('proud', 'republic')] \n",
        "#use 3 for trigrams\n",
        "trigrams = ngrams(vocab_no_punct,3)\n",
        "print(list(trigrams)) \n",
        "[('India', 'Indians', 'We'), ('Indians', 'We', 'a'), ('We', 'a', 'are'), ('a', 'are', 'country'), \n",
        " ('are', 'country', 'is'), ('country', 'is', 'proud'), ('is', 'proud', 'republic')]\n"
      ],
      "metadata": {
        "id": "hY6O8QbrNQ6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk import word_tokenize\n",
        "from nltk import wordpunct_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "#read the file \n",
        "#Note: use the correct path of the file depending on your environment\n",
        "file = open(\"nlp_wikipedia_sample.txt\",'r') \n",
        "text = ''\n",
        "for i in file.readlines():\n",
        "    text += i\n",
        "# print(text)\n",
        "#remove trailing spaces\n",
        "trimmed_text = text.strip()\n",
        "# print(trimmed_text)\n",
        "converted_text = trimmed_text.lower()\n",
        "# print(converted_text)\n"
      ],
      "metadata": {
        "id": "bOmEvFrYQ4-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import RegexpParser\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag\n",
        "barack = \"\"\"Barack Hussein Obama II born August 4, 1961) is an American politician\n",
        "who served as the 44th President of \n",
        "the United States from January 20, 2009, to January 20, 2017.\n",
        "A member of the Democratic Party, he was the \n",
        "first African American to assume the presidency and previously\n",
        "served as a United States Senator from Illinois (2005–2008).\"\"\"\n",
        "grammar = r\"\"\"Place: {<NNP><NNPS>+}\n",
        "           Date: {<NNP><CD><,><CD>}\n",
        "           Person: {<NNP>+}\n",
        "           \"\"\"\n",
        "tokenised_barack = word_tokenize(barack)\n",
        "pos_list = pos_tag(tokenised_barack)\n",
        "regParser = RegexpParser(grammar)\n",
        "reg_lines = regParser.parse(pos_list)\n",
        "print(reg_lines) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmKUC4Dxizy4",
        "outputId": "859dd99a-13a7-4bf4-e710-dce5e4a92ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (Person Barack/NNP Hussein/NNP Obama/NNP II/NNP)\n",
            "  born/VBD\n",
            "  (Date August/NNP 4/CD ,/, 1961/CD)\n",
            "  )/)\n",
            "  is/VBZ\n",
            "  an/DT\n",
            "  American/JJ\n",
            "  politician/NN\n",
            "  who/WP\n",
            "  served/VBD\n",
            "  as/IN\n",
            "  the/DT\n",
            "  44th/CD\n",
            "  (Person President/NNP)\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (Place United/NNP States/NNPS)\n",
            "  from/IN\n",
            "  (Date January/NNP 20/CD ,/, 2009/CD)\n",
            "  ,/,\n",
            "  to/TO\n",
            "  (Date January/NNP 20/CD ,/, 2017/CD)\n",
            "  ./.\n",
            "  A/DT\n",
            "  member/NN\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (Person Democratic/NNP Party/NNP)\n",
            "  ,/,\n",
            "  he/PRP\n",
            "  was/VBD\n",
            "  the/DT\n",
            "  first/JJ\n",
            "  African/JJ\n",
            "  (Person American/NNP)\n",
            "  to/TO\n",
            "  assume/VB\n",
            "  the/DT\n",
            "  presidency/NN\n",
            "  and/CC\n",
            "  previously/RB\n",
            "  served/VBD\n",
            "  as/IN\n",
            "  a/DT\n",
            "  (Place United/NNP States/NNPS)\n",
            "  (Person Senator/NNP)\n",
            "  from/IN\n",
            "  (Person Illinois/NNP)\n",
            "  (/(\n",
            "  2005–2008/CD\n",
            "  )/)\n",
            "  ./.)\n"
          ]
        }
      ]
    }
  ]
}